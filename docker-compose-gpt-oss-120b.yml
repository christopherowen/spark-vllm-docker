services:
  vllm:
    image: spark-vllm:latest
    restart: unless-stopped
    ports:
      - "8000:8000"

    environment:
      VLLM_USE_FLASHINFER_MOE_FP4: "1"
      VLLM_FLASHINFER_MOE_BACKEND: "throughput"

    runtime: nvidia
    ipc: host
    ulimits:
      memlock: -1
      stack: 67108864

    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
      - ./.cache/vllm:/root/.cache/vllm
      - ./.cache/torch:/root/.cache/torch
      - ./.cache/triton:/root/.cache/triton
      - ./.cache/flashinfer:/root/.cache/flashinfer
      - ./chat_templates/gpt-oss-120b.jinja:/workspace/chat_templates/gpt-oss-120b.jinja:ro
      - ./bench:/bench

    command: >
      vllm serve openai/gpt-oss-120b
        --host 0.0.0.0
        --port 8000
        --served-model-name gpt-oss-120b
        --override-generation-config '{"temperature":1.0,"top_p":1.0,"top_k":0}'
        --chat-template /workspace/chat_templates/gpt-oss-120b.jinja
        --enable-auto-tool-choice
        --tool-call-parser=openai
        --reasoning-parser=openai_gptoss
        --gpu-memory-utilization 0.70
        --max-model-len 131072
        --max-num-seqs 2
        --async-scheduling
        --max-num-batched-tokens 8192
        --enable-prefix-caching
        --load-format fastsafetensors
